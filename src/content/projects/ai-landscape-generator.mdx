---
title: "AI Procedural Landscape Generator"
description: "Thesis project combining Stable Diffusion with a custom C++ OpenGL renderer for AI-guided terrain generation."
publishedAt: 2024-08-16
status: completed
category: research
technologies: [C++, OpenGL, Python, Stable Diffusion, CUDA]
featured: true
cover: /images/projects/thesis/cover.jpg
links:
  github: https://github.com/VKG5/Procedural-Landscape-Generation
---

import Mermaid from '../../components/common/Mermaid.astro';

## Overview

This project aims to simplify landscape generation by overcoming the learning curve and limitations of traditional tools like World Machine, 
Gaea, and node-based workflows such as Blender Geometry Nodes and Houdini. It combines:

- **AI-Powered Generation**: Uses Stable Diffusion with custom-trained LoRA models to generate heightmaps.
- **Real-Time Rendering:** OpenGL-based C++ application for visualizing and rendering terrains.
- **User-Friendly GUI**: PyQt5 and ImGUI interface for easy interaction with the generation pipeline.
- **Dataset Collection**: Scripts for gathering and processing Digital Elevation Models (DEMs) from OpenTopography, custom data generated in Blender using Shaders, Geometry Nodes, and scripting.

The work was developed as part of my Masterâ€™s dissertation and targets real-world production constraints in games, VFX, and virtual environments.

## Research Question

This project investigates the use of **latent diffusion models for procedural terrain synthesis**, with a focus on generating high-quality heightmaps that can be directly converted into large-scale 
3D landscapes. The primary objective was to replace parameter-heavy, simulation-driven terrain workflows with a **data-driven**, **example-based system** that remains fast, controllable, and reproducible.

## Technical Motivation

Traditional procedural terrain pipelines rely on fractals, erosion solvers, and node-based graphs. While physically motivated, these approaches suffer from:
- High iteration cost
- Complex parameter spaces
- Limited scalability for large terrains

Recent GAN-based solutions improve automation but introduce instability, tiling artifacts, and large dataset requirements. This project explores Stable Diffusion fine-tuned for 
heightmap generation as a more **scalable** alternative, leveraging **latent space efficiency** and **transfer learning** to achieve higher fidelity with significantly lower training cost.

## System Architecture

The system is designed as a modular, tile-based generation pipeline that converts noise into heightmaps and then into real-time 3D terrain.

<Mermaid chart={`
flowchart TD
    A[ðŸ—ƒï¸ DEM Dataset] --> B[âš™ï¸ Preprocessing]
    B --> C[ðŸ¤– Stable Diffusion + LoRA]
    C --> D[ðŸ—ºï¸ Heightmap Tiles]
    D --> E[ðŸ§© Terrain Assembly]
    E --> F[ðŸŽ® 3D Rendering Engine]
    
    style A fill:#4FC3F7,stroke:#2B7A9E,color:#000
    style B fill:#4FC3F7,stroke:#2B7A9E,color:#000
    style C fill:#4FC3F7,stroke:#2B7A9E,color:#000
    style D fill:#4FC3F7,stroke:#2B7A9E,color:#000
    style E fill:#4FC3F7,stroke:#2B7A9E,color:#000
    style F fill:#4FC3F7,stroke:#2B7A9E,color:#000
`} />

### Stage 1: AI Generation
Using Stable Diffusion with custom landscape-focused fine-tuning to generate concept images. The key was training on images with clear horizon lines and consistent lighting.

### Stage 2: Feature Extraction
A Python pipeline extracts:
- Heightmap approximation (from luminance gradients)
- Biome classification (from color clusters)
- Vegetation density (from texture analysis)

### Stage 3: Terrain Reconstruction
The C++ engine reconstructs 3D terrain using:
- GPU-accelerated heightmap generation
- Procedural detail overlay (rocks, grass, trees)
- Atmospheric scattering for consistent lighting

## Results

The system successfully generates coherent 3D landscapes from AI concepts, with some caveats:
- Works best with landscapes that have clear depth cues
- Struggles with highly stylized or abstract inputs
- Generation quality depends heavily on the source image

## Technical Challenges

1. **Depth estimation** â€” 2D to 3D reconstruction is inherently ambiguous
2. **Style consistency** â€” Matching procedural detail to AI aesthetics
3. **Performance** â€” Real-time rendering while processing AI features

## Outcome

- Successfully defended thesis
- Paper submitted to SIGGRAPH Asia (under review)
- Open-sourced the renderer and feature extraction pipeline

## Future Work

- Real-time Stable Diffusion integration (currently batch-processed)
- Multi-view synthesis for more accurate depth
- Vegetation and structure placement using AI suggestions
