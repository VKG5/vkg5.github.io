[["Map",1,2,9,10,33,34],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.4","content-config-digest","4960f797457ae201","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://varungupta.dev\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","projects",["Map",11,12],"ai-landscape-generator",{"id":11,"data":13,"body":29,"filePath":30,"digest":31,"legacyId":32,"deferredRender":25},{"title":14,"description":15,"publishedAt":16,"status":17,"category":18,"technologies":19,"featured":25,"links":26,"cover":28},"AI Procedural Landscape Generator","Thesis project combining Stable Diffusion with a custom C++ OpenGL renderer for AI-guided terrain generation.",["Date","2024-08-16T00:00:00.000Z"],"completed","research",[20,21,22,23,24],"C++","OpenGL","Python","Stable Diffusion","CUDA",true,{"github":27},"https://github.com/VKG5/Procedural-Landscape-Generation","/images/projects/thesis/cover.jpg","import Mermaid from '../../components/common/Mermaid.astro';\r\n\r\n## Overview\r\n\r\nThis project aims to simplify landscape generation by overcoming the learning curve and limitations of traditional tools like World Machine, \r\nGaea, and node-based workflows such as Blender Geometry Nodes and Houdini. It combines:\r\n\r\n- **AI-Powered Generation**: Uses Stable Diffusion with custom-trained LoRA models to generate heightmaps.\r\n\r\n\u003Cimg src=\"/images/projects/thesis/landscapePreview.png\" class=\"w-full rounded-lg border border-border\" />\r\n\r\n- **Real-Time Rendering:** OpenGL-based C++ application for visualizing and rendering terrains.\r\n\r\n- **User-Friendly GUI**: PyQt5 and ImGUI interface for easy interaction with the generation pipeline.\r\n\r\n\u003Cimg src=\"/images/projects/thesis/UIBasic.png\" class=\"w-full rounded-lg border border-border\" />\r\n\r\n- **Dataset Collection**: Scripts for gathering and processing Digital Elevation Models (DEMs) from OpenTopography, custom data generated in Blender using Shaders, Geometry Nodes, and scripting.\r\n\r\nThe work was developed as part of my Master’s dissertation and targets real-world production constraints in games, VFX, and virtual environments.\r\n\r\n## Research Question\r\n\r\nThis project investigates the use of **latent diffusion models for procedural terrain synthesis**, with a focus on generating high-quality heightmaps that can be directly converted into large-scale \r\n3D landscapes. The primary objective was to replace parameter-heavy, simulation-driven terrain workflows with a **data-driven**, **example-based system** that remains fast, controllable, and reproducible.\r\n\r\n## Technical Motivation\r\n\r\nTraditional procedural terrain pipelines rely on fractals, erosion solvers, and node-based graphs. While physically motivated, these approaches suffer from:\r\n- High iteration cost\r\n- Complex parameter spaces\r\n- Limited scalability for large terrains\r\n\r\nRecent GAN-based solutions improve automation but introduce instability, tiling artifacts, and large dataset requirements. This project explores Stable Diffusion fine-tuned for \r\nheightmap generation as a more **scalable** alternative, leveraging **latent space efficiency** and **transfer learning** to achieve higher fidelity with significantly lower training cost.\r\n\r\n## System Architecture\r\n\r\nThe system is designed as a modular, tile-based generation pipeline that converts noise into heightmaps and then into real-time 3D terrain.\r\n\r\n\u003CMermaid chart={`\r\nflowchart TD\r\n    A[DEM Dataset] --> B[Preprocessing]\r\n    B --> C[Stable Diffusion + LoRA]\r\n    C --> D[Heightmap Tiles]\r\n    D --> E[Terrain Assembly]\r\n    E --> F[3D Rendering Engine]\r\n    \r\n    style A fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style B fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style C fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style D fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style E fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style F fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n`} />\r\n\r\n## Component Breakdown\r\n### 1. Data & Representation Layer\r\n\r\n#### Input Data\r\n- Digital Elevation Models (DEMs) sourced from OpenTopography and curated datasets.\r\n- Converted to **single-channel heightmaps** (grayscale), where luminance encodes elevation.\r\n\r\n#### Why Heightmaps?\r\n- GPU-friendly and memory efficient\r\n- Engine-agnostic (Unreal, Unity, Godot, custom engines)\r\n- Direct mapping to vertex displacement\r\n\r\nHeightmaps serve as the system’s **canonical** representation, decoupling generation from rendering.\r\n\r\n### 2. Diffusion Model Architecture\r\n\r\nThe core generator is a **latent diffusion model (Stable Diffusion)** composed of:\r\n\r\n\u003CMermaid chart={`\r\nflowchart TD\r\n    A[Image / Noise] --> B[VAE Encoder]\r\n    B --> C[Latent Space]\r\n    C --> D[U-Net & Cross Attenuation]\r\n    D --> E[VAE Decoder]\r\n    E --> F[Heightmap Output]\r\n    \r\n    style A fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style B fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style C fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style D fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style E fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style F fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n`} />\r\n\r\n- **VAE (Variational Autoencoder)** compresses heightmaps into a latent space, reducing compute cost.\r\n- **U-Net** performs iterative denoising, learning terrain structure and spatial relationships.\r\n- **Cross-attention** enables conditioning using text prompts or guiding images.\r\n\r\n### 3. LoRA Fine-Tuning Strategy\r\n\r\nInstead of retraining the full diffusion model:\r\n\r\n- Base Stable Diffusion weights are frozen\r\n- **Low-Rank Adaptation (LoRA)** layers are injected into cross-attention blocks\r\n- Only rank-decomposed matrices are trained\r\n\r\n#### Benefits\r\n- Training possible with **20–100 samples**\r\n- Model sizes reduced to MBs instead of GBs\r\n- Enables rapid experimentation on consumer GPUs\r\n\r\nThis makes the system practical for **indie teams**, **research prototyping**, and **tool development**.\r\n\r\n### 4. Tile-Based Generation System\r\n\r\nLarge terrains are generated using overlapping tiles, enabling scalability without exhausting GPU memory.\r\n\r\n\u003CMermaid chart={`\r\nflowchart TD\r\n    A[Global Seed] --> B[Tile Coordinates]\r\n    B --> C[Conditional Diffusion]\r\n    C --> D[Seam-aware Heightmap Tiles]\r\n    \r\n    style A fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style B fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style C fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n    style D fill:#4FC3F7,stroke:#2B7A9E,color:#000\r\n`} />\r\n\r\n- **Neighbor-aware** conditioning reduces seams\r\n- **Deterministic seeds** allow exact regeneration\r\n- **Individual tiles** can be **re-generated** without invalidating the entire terrain\r\n\r\nThis mirrors how open-world games stream terrain data at runtime. Similar to [World Partitions](https://dev.epicgames.com/documentation/en-us/unreal-engine/world-partition-in-unreal-engine) in Unreal Engine 5.\r\n\r\n#### 5. Terrain Reconstruction & Rendering\r\n\r\nGenerated heightmaps are converted into 3D terrain via:\r\n- High-density mesh tessellation\r\n- Vertex displacement from height values\r\n- Procedural normal generation\r\n- UV mapping and shading\r\n\r\nA **custom lightweight renderer** was implemented to:\r\n- Minimize iteration time\r\n- Maintain control over mesh density and precision\r\n- Visualize results without full engine overhead\r\n\r\n## Why?\r\n\r\nThis system enables **scalable world generation** by allowing large terrains to be prototyped and regenerated incrementally, with **deterministic results** suitable for production pipelines. \r\nAI is used as a front-loaded generator that artists guide through prompts, masks, and seeds, ensuring **creative control** rather than automation. By operating on heightmaps, the workflow \r\nintegrates directly with existing engines and tools, while remaining extensible to related data such as material maps, foliage density, and biome masks. Positioning diffusion-based \r\nPCG as a general-purpose foundation for world building.\r\n\r\n## Key Takeaways\r\n- Latent diffusion models **outperform GANs** for procedural terrain synthesis\r\n- Example-based learning removes the need for **explicit geological simulation**\r\n- Tile-aware diffusion enables **scalable open-world generation**\r\n- AI-assisted PCG works best as a **creative accelerator**, not a black box\r\n\r\nBelow are some generated results:\r\n\r\n\u003Cimg src=\"/images/projects/thesis/resultsW.png\" class=\"w-full rounded-lg border border-border\" />\r\n\r\n## TL;DR\r\nBuilt a diffusion-based procedural terrain system that generates production-ready heightmaps in seconds using Stable Diffusion fine-tuned with LoRA. \r\nThe pipeline scales to large worlds via tile-based generation, produces deterministic results for iteration and version control, and integrates directly with \r\nexisting game engines through standard heightmap workflows. Designed as an artist-guided tool, not a black box, enabling rapid prototyping without sacrificing control.\r\n\r\n---\r\n\r\n*Have any more questions? Find me on Twitter or shoot me an email :)*","src/content/projects/ai-landscape-generator.mdx","19ba9510d37ef6d0","ai-landscape-generator.mdx","blog",["Map",35,36],"shaders-procedural-clouds-01",{"id":35,"data":37,"body":50,"filePath":51,"digest":52,"legacyId":53,"deferredRender":25},{"title":38,"description":39,"publishedAt":40,"category":41,"tags":42,"series":41,"seriesOrder":48,"featured":25,"draft":49},"Procedural 2D Clouds : A mathematical approach to nature","A deep dive into generating procedural clouds using maths, followed by some tips to make them appear volumetric.",["Date","2025-12-17T00:00:00.000Z"],"shaders",[43,41,44,45,46,47],"tutorial","glsl","clouds","pcg","noise",1,false,"One of the biggest mistakes I made while developing this shader was not considering the computational cost. This cost me 2 days in live-production and a huge headache, but everything worked out\r\nin the end!\r\n\r\nClouds are something that have fascinated me, you, and possibly everyone at one point in their life. Even though they appear 2D in the sky, they actually have a lot of volume and mass to them.\r\nThis also means they are computationally heavy to render/generate. My target was to use something that was NOT a volume, and what else would work if not noise! I have a separate blog for different\r\nnoises lined up, so keep out an eye ;)\r\n\r\nIn this breakdown, I'll walk through a texture-based, optimized 2D cloud shader I wrote in Godot. It's the system I developed for a recent project at Zitro, and it handles a huge amount of clouds\r\nwhile maintaining performance. The is not a \"cool clouds\" shader, rather performance-centric.\r\n\r\n\r\n## Non-Procedural Noise\r\n\r\nThe shader is built around a single-texture with **multiple textures packed** into **different channels**. If you are new to image processing, a typical image is usually comprised of `RGB channels`, usually \r\nrepresented by `JPEG`. Often times there is a fourth one as well called `Alpha`, making the image `RGBA`, with the following extensions `PNG, EXR, TARGA, TIFF`. For this shader, the channels are as\r\nfollows:\r\n\r\n- **Red Channel (R)**: Static shape mask\r\n- **Green Channel (G)**: Tiled, primary scrollable noise\r\n- **Blue Channel (B)**: Tiled, secondary scrollable noise\r\n\r\n### Why?\r\n\r\nWell, this way we avoid any expensive procedural noise functions and instead rely on **texture lookups**. These are almost always quicker, and since we have all the data in one image (AKA Texture), we only\r\nneed one lookup and one load cycle. We can simply access the various channels/sub-textures using **swizzling** in **GLSL (OpenGL Shading Language)**. This is a shorthand notation of extracting data from a container data type such as vec3.\r\n\r\n```glsl\r\nvoid fragment() {\r\n    vec4 tex = texture(TEXTURE, UV);\r\n\r\n    // Getting red, green, blue channels\r\n    vec3 tex_rgb = tex.rgb;\r\n\r\n    // Getting individual channels\r\n    float red = tex.r;\r\n    float green = tex.g;\r\n    float blue = tex.b;\r\n}\r\n```\r\n\r\nYou can grab the image below. The funky colours are due to different data present in each channel.\r\n\r\n\u003Cimg src=\"/images/blog/pcg_clouds_2d/clouds_texture.png\" class=\"w-full rounded-lg border border-border\" />\r\n\r\n\u003Cdiv class=\"grid grid-cols-3 gap-4 my-8\">\r\n    \u003Cdiv>\r\n        \u003Cimg src=\"/images/blog/pcg_clouds_2d/clouds_texture_r.png\" alt=\"Red channel - Static shape mask\" class=\"w-full rounded-lg border border-border\" />\r\n        \u003Cp class=\"text-sm text-center mt-2\" style=\"color: var(--color-text-tertiary)\">Red: Shape Mask\u003C/p>\r\n    \u003C/div>\r\n    \u003Cdiv>\r\n        \u003Cimg src=\"/images/blog/pcg_clouds_2d/clouds_texture_g.png\" alt=\"Green channel - Primary scrollable noise\" class=\"w-full rounded-lg border border-border\" />\r\n        \u003Cp class=\"text-sm text-center mt-2\" style=\"color: var(--color-text-tertiary)\">Green: Primary Noise\u003C/p>\r\n    \u003C/div>\r\n    \u003Cdiv>\r\n        \u003Cimg src=\"/images/blog/pcg_clouds_2d/clouds_texture_b.png\" alt=\"Blue channel - Secondary scrollable noise\" class=\"w-full rounded-lg border border-border\" />\r\n        \u003Cp class=\"text-sm text-center mt-2\" style=\"color: var(--color-text-tertiary)\">Blue: Secondary Noise\u003C/p>\r\n    \u003C/div>\r\n\u003C/div>\r\n\r\nIf you use [Unreal Engine](https://www.unrealengine.com/en-US)/[Quixel](https://quixel.com/en-US), or any major asset shop, this is a very common technique to save texture lookups and memory. \r\nI got introduced to the concept a few years ago, and I haven't been able to go back, it is truly the simplest optimization for any game.\r\n\r\n\r\n## Uniforms - Control Without Complexity\r\n\r\nAlright, now we know how to sample a texture. But how do we pass it on to the GPU? \r\n\r\n**Uniforms!**\r\n\r\nUniforms are special variables that act as a bridge between the CPU (Game Engine) and GPU (Shader). They are read-only parameters that remain constant across all vertices or fragments\r\nin a single draw call. Shaders run in parallel on thousands of pixels/vertices simulataneously. They cannot access you script variables directly as they are running on an entirely different\r\nprocessing environment, the GPU. Uniforms solve this by:\r\n\r\n- **Passing data once**: You set a uniform value on the CPU, and every shader invocation sees the same value\r\n- **Staying constant**: Unlike varying variables (which interpolate between vertices), uniforms don't change during a single render pass\r\n- **Being efficient**: One upload, many reads across all parallel threads\r\n\r\n### Common Use Cases\r\n\r\n- **Textures**: Passing texture samplers (like our shape mask and noise textures)\r\n- **Colors**: Theme colors, fog colors, etc.\r\n- **User settings & custom parameters**: Slider values, toggle states\r\n\r\nIn our case, we'll use a uniform to pass our noise texture and shape mask to the fragment shader, making it accessible for cloud generation. Another great thing about uniforms is that you\r\ncan make changes after compilation. A simple comaprison for uniforms can be made with **Material Instance** in Unreal Engine. The shader is **compiled once** and you can have **multiple variations** \r\nfor the same using uniforms. Analogous to **polymorphism** in **Object Oriented Programming (OOPs)**.\r\n\r\n\r\n## UV & Motion Controls\r\n\r\n```glsl\r\n// Uniforms\r\nuniform vec2 uv_scale = vec2(100, 75);\r\n\r\n// Wind control\r\nuniform vec2 direction = vec2(1.0, 0.0);\r\nuniform float cloud_scale = 1.5;\r\nuniform float speed : hint_range(-1.0, 1.0) = 0.03;\r\n```\r\n\r\nThese uniforms define the spatial scale and motion of the clouds.\r\n\r\n`uv_scale` and `cloud_scale` is used to scale the UVs. This can cause stretching/squashing along with scaling, which is desired for clouds.\r\n\r\n`direction` and `speed` together define wind movement. They have been separated for granular control over both aspect.\r\n\r\n\r\n## Lighting & Composition Controls\r\n\r\n```glsl\r\n// Artistic Controls\r\nuniform float cloud_dark = 0.5;\r\nuniform float cloud_light = 0.3;\r\nuniform float cloud_cover : hint_range(-10.0, 1.0) = 0.2;\r\nuniform float cloud_alpha : hint_range(0.0, 3.0) = 0.25;\r\nuniform float sky_tint : hint_range(0.0, 1.0) = 0.6;\r\n\r\n// Sky Hue\r\nuniform vec4 sky_colour_01 : hint_color = vec4(0.2, 0.4, 0.6, 1.0);\r\nuniform vec4 sky_colour_02 : hint_color = vec4(1.0, 0.647, 0.0, 1.0);\r\n```\r\n\r\nThese parameters are **artist-friendly**. As a technical artist and graphics engineer, it is my job to make these shaders/tools **accessible** to the artists.\r\nParameters are a great way of doing so. I sat down with an artist and managed to get some incredible results based on their tweaking! The eye and experience\r\nof an artist should be respected more than it is nowadays.\r\n\r\nInstead of **physically-based lighting**, I use simple scalar parameters to control various aspects of the cloud. This ensures smooth performance in real-time\r\nand avoids reworking noise functions or the shader.\r\n\r\n\r\n## Vertex Shader - Why It Exists Here?\r\n\r\nThe vertex shader runs only **four-times for a quad**, once per vertex. I am using a simple plane to render out the clouds for our project. This is helpful when\r\nyou look at the holistic picture. Our target resolution is native `2160 x 6000`, sometimes even more. Considering I sample the noise texture multiple times, \r\nan average of 15 per fragment, the total number of times I need to sample the texture would be:\r\n\r\n```\r\n2160 x 6000 = 12,960,000 (Total fragments)\r\n12,960,000 x 15 = 194,400,000 (Texture Samples)\r\n```\r\n\r\nPhew! That is a VERY large number. Thanks to modern GPUs, these computations do not mean a lot, but if your target is mobile devices, or weaker hardware, this becomes\r\na problem. *Like it did for me, while using procedural noise (Oh yes, the count was almost 30x than this).*\r\n\r\nThe vertex shader helps reduce some calculations that will remain constant throughout the lifetime of the shader.\r\n\r\n```glsl\r\n// flat varying vec2 cloud_uv : No interpolation, use vertex A's value\r\nvarying vec2 cloud_uv;\r\nvarying vec2 time_vec;\r\nvarying float aspect_ratio;\r\n\r\nvoid vertex() {\r\n\taspect_ratio = (uv_scale.y != 0.0) ? (uv_scale.x / uv_scale.y) : 1.0;\r\n\r\n\t// Fix aspect ratio so clouds look consistent on different screen sizes\r\n\tcloud_uv = UV * vec2(aspect_ratio, 1.0) * cloud_scale;\r\n\r\n\t// TIME for animating the clouds\r\n\ttime_vec = TIME * speed * direction;\r\n}\r\n```\r\n\r\nAnything that doesn't need per-pixel precision, such as *aspect ratio* or time-based offsets, is calculated here and passed down as varyings.\r\n\r\nWhile this may look like an insignificant optimization, but in shaders that are rendered across large screens, these choices add up.\r\n\r\n### Understanding the `varying` keyword\r\n\r\nThe `varying` keyword is a bridge between vertex shader and fragment shader. It gets calculated once per vertex and then gets *interpolated* across the entire surface for every\r\nfragment (pixel) that gets rendered. Consider the following code snipped:\r\n\r\n```glsl\r\n// VERTEX SHADER (runs 4 times for a quad)\r\n// Computes and send interpolated value to fragment shader\r\nvarying vec2 cloud_uv;\r\n\r\nvoid vertex() {\r\n    // This runs ONLY 4 times (once per corner of the quad)\r\n    cloud_uv = UV * vec2(aspect_ratio, 1.0) * cloud_scale;\r\n}\r\n\r\n// FRAGMENT SHADER (runs millions of times)\r\nvarying vec2 cloud_uv;\r\n\r\nvoid fragment() {\r\n    // cloud_uv is automatically interpolated between the 4 vertex values\r\n    // So every pixel gets a smoothly blended value\r\n}\r\n```\r\n\r\nWhen the GPU processed a quad (rectangle):\r\n\r\n1. **Vertex Shader** calculates `cloud_uv` at each of the 4 vertices (corners)\r\n    - Top-left corner: cloud_uv = (0.0, 1.0)\r\n    - Top-right corner: cloud_uv = (1.0, 1.0)\r\n    - Bottom-left: cloud_uv = (0.0, 0.0)\r\n    - Bottom-right: cloud_uv = (1.0, 0.0)\r\n\r\n2. The GPU automatically interpolates these values across the surface\r\n    - A pixel in the center gets cloud_uv ≈ (0.5, 0.5)\r\n    - A pixel 25% from the left gets cloud_uv ≈ (0.25, y)\r\n\r\n3. Fragment Shader receives the interpolated value for each pixel and runs the `void fragment()` code on each pixel.\r\n\r\nHaving mentioned **texture lookup** and sampling multiple times, what do they actually mean?\r\n\r\n### Texture Lookup & Sampling\r\n\r\nThe act of fetching a particular pixel's colour (texel) from a texture using UV coordinates. Instead of computing noise\r\nmathematically, the function samples noise directly from a texture channel (As shown above).\r\n\r\nThe green and blue channels provide two independent noise sources,  which can be layered and animated separately. This\r\nkeeps the shader fast and predictable across hardware. Why predictable? Each system's RNG (Random Number Generator) will\r\nproduce a different value, which may cause artifacts or broken textures. The code below is used to sample noise based on\r\na parameter (channel).\r\n\r\n```glsl\r\nfloat texture_noise(sampler2D tex, vec2 p, int channel) {\r\n\tif(channel == 0) {\r\n\t\treturn texture(tex, p).g;\r\n\t} else {\r\n\t\treturn texture(tex, p).b;\r\n\t}\r\n}\r\n```\r\n\r\nTexture sampling is the process of using interpolated UV coordinates in the fragment shader to fetch texel values from\r\na texture. A single texture lookup returns all channels (RGBA) so you can swizzle out the green and blue channels as\r\nindependent noise sources for layering and animation. Swizzling is a shorthand in GLSL where you can access various\r\nchannels in a container data-type by using:\r\n\r\n```glsl\r\nvec3 pos = vec3(0.1, -10.3, 4.0);\r\n\r\n// Extracting the X, Y, and Z channels and their combinations from a predefined vector\r\nfloat a = pos.x; // The resultant value will be 0.1\r\nvec2 b = pos.yx; // The resultant vector will be (-10.3, 0.1)\r\nvec3 c = pos.zzy; // The resultant vector will be (4.0, 4.0, -10.3)\r\n```\r\n\r\nSwizzling provides you an easy way to grab different components of a data container (for example: color.rgb, pos.xy or \r\npos.zzx), allowing concise access, reordering and replication of channels.\r\n\r\n### Fractal Brownian Motion (FMB)\r\n\r\nAdding different iterations of noise *(octaves)*, where we sample the noise at different scales, rotations, and decreasing amplitude. This way\r\nwe get more granulatiry in the noise and get more fine detail. This technique is called **Fractal Browning Motion (fBM), or simply,\r\nfractal noise. The following code achieves the same:\r\n\r\n```glsl\r\n// Rotation matrix for FBM layers\r\nconst mat2 m = mat2(vec2(1.6, 1.2), vec2(-1.2, 1.6));\r\n\r\nfloat fbm(sampler2D tex, vec2 n, int channel) {\r\n\tfloat total = 0.0, amplitude = 0.1;\r\n\tfor(int i=0; i \u003C 4; i++) {\r\n\t\ttotal += texture_noise(tex, n, channel) * amplitude;\r\n\t\tn = m * n;\r\n\t\tamplitude *= 0.4;\r\n\t}\r\n\treturn total;\r\n}\r\n```\r\n\r\nHere `m` is a constant matrix, which helps break visible tiling while keeping the math minimal.\r\n\r\n### Animated Noise\r\n\r\nThis function builds animated noise by repeatedly samplign the texture while progressively altering UVs and weights. This function is the\r\n*heart* of the shader.\r\n\r\nBy exposing various parameters, the same function can be reused for:\r\n- Cloud Shape\r\n- Detail Breakup\r\n- Colour Variation\r\n\r\nFor our use case, we have used a extracted the colours extracts from the sky. This is done in the `void fragment()` function. Below you can find the\r\nnoise animation function that I have used for generating the clouds:\r\n\r\n```glsl\r\n// Optimized animation function using texture lookups\r\nfloat animate_noise(sampler2D tex, int channel, vec2 time_offset, vec2 uv, float q, float uv_step, float weight, float weight_step, int steps) {\r\n\tfloat val = 0.0;\r\n\tvec2 uv2 = uv * uv_step - q + time_offset;\r\n\t\r\n\tfor(int i = 0; i \u003C steps; i++) {\r\n\t\tval += abs(weight * texture_noise(tex, uv2, channel));\r\n\t\tuv2 = m * uv2 + time_offset;\r\n\t\tweight *= weight_step;\r\n\t}\r\n\t\r\n\treturn val;\r\n}\r\n```\r\n\r\n## Fragment Shader - Putting it All Together\r\n\r\nBelow is a detailed breakdown of the fragment shader used. \r\n\r\n### Masking\r\n\r\nThe red channel of our sampled texture defines a mask where the clouds are allowed to exist. This keeps the edges clean, smooth, \r\nand avoids procedural noise bleeding outside the intended shape.\r\n\r\n```glsl\r\nfloat mask = texture(TEXTURE, UV).r;\r\n```\r\n\r\nUsing the shape is as simple as multiplying the alpha of the final image with this mask.\r\n\r\n```glsl\r\nCOLOR.a *= mask;\r\n```\r\n\r\nThis circumvents the need to write any edge blending logic and blurring/lowering opacity; at the cost of some memory. Below you\r\ncan see the difference between having a mask and not. The left image (without mask) shows harsh edges and seams between textures.\r\nThe right image (with mask) solves this issue, giving a much more natural result.\r\n\r\n\u003Cdiv class=\"grid grid-cols-2 gap-4 my-8\">\r\n    \u003Cdiv>\r\n        \u003Cimg src=\"/images/blog/pcg_clouds_2d/without_mask.png\" alt=\"Without Mask - Clouds rendered on a plane without applying mask\" class=\"w-full rounded-lg border border-border\" />\r\n        \u003Cp class=\"text-sm text-center mt-2\" style=\"color: var(--color-text-tertiary)\">Without Mask\u003C/p>\r\n    \u003C/div>\r\n    \u003Cdiv>\r\n        \u003Cimg src=\"/images/blog/pcg_clouds_2d/with_mask.png\" alt=\"With Mask - Clouds rendered on a plane with the mask\" class=\"w-full rounded-lg border border-border\" />\r\n        \u003Cp class=\"text-sm text-center mt-2\" style=\"color: var(--color-text-tertiary)\">With Mask\u003C/p>\r\n    \u003C/div>\r\n\u003C/div>\r\n\r\n### Noise Construction\r\n\r\nI first build a base FBM signal `q`, then layer ridged and smooth noise on top of it. By sampling different noise textures between each noise sample, it gives more natural,\r\nrandomized results, breaking tiling, and adding visual complexity.\r\n\r\n```glsl\r\n// 0. Base FBM (Pass TEXTURE sampler)\r\nfloat q = fbm(TEXTURE, cloud_uv * 0.5, 0);\r\n```\r\n\r\nHere, a low-frequency FBM noise at half the UV scale is generated. This is used to **wrap/distort** UV coordinates of subsequent noise layers, adding organic irregularity. The `0` means\r\nit samples the **green channel**.\r\n\r\n```glsl\r\n// 1. Ridged Noise Shape (Pass TEXTURE sampler)\r\nfloat r = animate_noise(TEXTURE, 0, time_vec, cloud_uv, q, 1.0, 0.8, 0.7, 4);\r\n```\r\n\r\nThis call generates the primary clouds shape using ridged noise (`abs()` in `animate_noise`). This means that the valleys become peaks, giving sharp clouds edges.\r\n\r\n```glsl\r\n// 2. Noise Shape (Pass TEXTURE sampler)\r\nfloat f = animate_noise(TEXTURE, 1, time_vec * 2.0, cloud_uv, q, 1.0, 0.7, 0.6, 4);\r\n\r\nf *= r + f;\r\n```\r\n\r\nThis function generated secondary detail noise that moves twice as fast, `time_vec * 2.0`. It samples the noise in the blue channel, and has a slightly softer falloff. I am then combining\r\nboth the noise layers non-linearly:\r\n\r\n- `r + f` = Ridged shape + detail noise.\r\n- `f *= (r + f)` = Detail noise is **amplified** where clouds exist.\r\n\r\nThis creates **denser detail in cloud areas** and fade details in empty regions. It's a cheap way to fake volumetri density, clouds look thicker and more detailed in their centers.\r\n\r\n```glsl\r\n// 3. Noise Colour (Pass TEXTURE sampler)\r\nfloat c = animate_noise(TEXTURE, 0, time_vec * 5.0, cloud_uv, q, 2.0, 0.4, 0.6, 3);\r\n```\r\n\r\nThis generates a fast-moving, high-frequency noise for colour/brightness variation. This moved 5x faster than base clouds (`time_vec * 5.0`), creating a shimmer effect. This adds **internal cloud lighing variation**,\r\nsame parts darker, some brighter.\r\n\r\nEach layer runs at a different speed and scale, which creates the illusion of depth and evolving volume.\r\n\r\n### Colour & Composition\r\n\r\nThe sky gradient is vertical and intentionally simple. The focus of the shader is motion and silhouette, not atmosphere scattering. *I am pretty interested in this as well, maybe a future blog?!*\r\n\r\nThe `mix()` function is used to generate a gradient, simply based on 2 uniform colours.\r\n\r\n```glsl\r\nuniform vec4 sky_colour_01 : hint_color = vec4(0.2, 0.4, 0.6, 1.0);\r\nuniform vec4 sky_colour_02 : hint_color = vec4(1.0, 0.647, 0.0, 1.0);\r\n\r\nvoid fragment() {\r\n    vec3 sky_colour = mix(sky_colour_02.rgb, sky_colour_01.rgb, UV.y);\r\n}\r\n```\r\n\r\nHere `UV.y = 0` is the bottom and render `sky_colour_02`; `UV.y = 1`, the top renders `sky_colour_01`. The `mix()` function **linearly** interpolates between them.\r\n\r\n```glsl\r\nvec3 cloud_colour = vec3(1.1, 1.1, 0.9) * clamp((cloud_dark + cloud_light * c), 0.0, 1.0);\r\n```\r\n\r\nUsing the `cloud_colour`, I get **subtle internal brightness variation**. The uniforms defined on top control the look and feel for the same.\r\n\r\n```glsl\r\nuniform float cloud_dark = 0.5;\r\nuniform float cloud_light = 0.3;\r\nuniform float cloud_cover : hint_range(-10.0, 1.0) = 0.2;\r\nuniform float cloud_alpha : hint_range(0.0, 3.0) = 0.25;\r\n\r\nvoid fragment() {\r\n    f = cloud_cover + cloud_alpha * f * r;\r\n}\r\n```\r\n\r\nI am recalculating the cloud density `f`. The calculations above control **how much clouds exist** at each pixel.\r\n- `cloud_cover` = Base amount of clouds (negative = less clouds).\r\n- `cloud_alpha * f * r` = Shape noise × ridged noise, scaled by alpha.\r\n\r\n```glsl\r\nfloat alpha = clamp(f + c, 0.0, 1.0);\r\nvec3 result = mix(sky_colour, clamp(sky_tint * sky_colour + cloud_colour, 0.0, 1.0), alpha);\r\n```\r\n\r\n`result` gives us the final `rgb` colours for our clouds. This takes into accoun the sky's colour. `sky_tint` controls the tint factor on the clouds. The final mix is done using the calculated `alpha`, which\r\nis the cloud coverage mask. Wherever `f + c` is high, show clouds and vice versa.\r\n\r\nFinally the end result is achieved by combining the resultant colour and alphas into a `vec4`.\r\n\r\n```glsl\r\nCOLOR = vec4(result, alpha * mask);\r\n```\r\n\r\n### Important Note!\r\n\r\nThis shader only renders the clouds and not the sky background. If you want to render the sky in the background as well, then you have to modify the final output to the following:\r\n\r\n```glsl\r\nCOLOR = vec4(result, mask);\r\n```\r\n\r\nWhat this does is use the mask as the alpha. You can also use `COLOR = vec4(result, 1.0)`, if you don't want a soft falloff to the scene.\r\n\r\n## Optimization (Viewport Rendering, Upscaling)\r\n\r\nThis is by-far one of the most important section of this breakdown. Rendering at the local `2160 x 6000` resolution was causing a huge\r\nperformance drop. The solution? Very simple, and probably intuitive. In this age of AI and upscaling, we will use the same. I am using\r\nGodot, which is an amazing open-source engine, for this task. The same concept can be used in any game engine using **Render Targets**.\r\nRender Targets are Dynamic 2D Textures, whose content can be updated during runtime. They are useful for a bunch of things, for example,\r\na painting/drawing board in game. In Godot the hierarchy is as follows:\r\n\r\n\u003Cdiv className=\"my-8 p-6 rounded-lg border border-border\" style={{backgroundColor: 'var(--color-bg-secondary)'}}>\r\n    \u003Cdiv className=\"flex flex-col items-center gap-6\">\r\n        {/* Root Node */}\r\n        \u003Cdiv className=\"flex flex-col items-center\">\r\n            \u003Cdiv className=\"px-6 py-3 rounded-lg shadow-md border-2\" style={{backgroundColor: 'rgba(47, 195, 247, 0.1)', borderColor: 'var(--color-accent)'}}>\r\n                \u003Cspan className=\"font-semibold text-lg\" style={{color: 'var(--color-accent)'}}>Node2D\u003C/span>\r\n            \u003C/div>\r\n            \u003Cdiv className=\"w-0.5 h-8\" style={{backgroundColor: 'var(--color-border)'}}>\u003C/div>\r\n        \u003C/div>\r\n\r\n        {/* First Level Children */}\r\n        \u003Cdiv className=\"flex gap-12\">\r\n            {/* TextureRect Branch */}\r\n            \u003Cdiv className=\"flex flex-col items-center\">\r\n                \u003Cdiv className=\"w-0.5 h-6\" style={{backgroundColor: 'var(--color-border)'}}>\u003C/div>\r\n                \u003Cdiv className=\"px-5 py-2.5 rounded-lg shadow-sm border-2 transition-colors hover:border-accent\" style={{backgroundColor: 'var(--color-bg-primary)', borderColor: 'var(--color-border)'}}>\r\n                    \u003Cspan className=\"font-medium\" style={{color: 'var(--color-text-primary)'}}>TextureRect\u003C/span>\r\n                \u003C/div>\r\n            \u003C/div>\r\n\r\n            {/* Viewport Branch */}\r\n            \u003Cdiv className=\"flex flex-col items-center\">\r\n                \u003Cdiv className=\"w-0.5 h-6\" style={{backgroundColor: 'var(--color-border)'}}>\u003C/div>\r\n                \u003Cdiv className=\"px-5 py-2.5 rounded-lg shadow-sm border-2\" style={{backgroundColor: 'var(--color-bg-primary)', borderColor: 'var(--color-border)'}}>\r\n                    \u003Cspan className=\"font-medium\" style={{color: 'var(--color-text-primary)'}}>Viewport\u003C/span>\r\n                \u003C/div>\r\n                \u003Cdiv className=\"w-0.5 h-8\" style={{backgroundColor: 'var(--color-border)'}}>\u003C/div>\r\n                \r\n                {/* Clouds Child */}\r\n                \u003Cdiv className=\"px-5 py-2.5 rounded-lg shadow-sm border-2\" style={{backgroundColor: 'rgba(47, 195, 247, 0.05)', borderColor: 'rgba(47, 195, 247, 0.5)'}}>\r\n                    \u003Cspan className=\"font-medium\" style={{color: 'var(--color-accent)'}}>Clouds.tscn\u003C/span>\r\n                \u003C/div>\r\n            \u003C/div>\r\n        \u003C/div>\r\n    \u003C/div>\r\n\r\n    \u003Cp className=\"text-sm text-center mt-6\" style={{color: 'var(--color-text-tertiary)'}}>\r\n        Godot Scene Hierarchy for Cloud Rendering\r\n    \u003C/p>\r\n\u003C/div>\r\n\r\nHere `Node2D` is the root of the scene, which acts as a container for all children node. In the texture rect I have created a `ViewportTexture`, which renders the `Viewport` node in a larger size. Godot\r\nprovides basic anti-aliasing algorithms such as **FXAA, MSAA**. This is a very basic method for upscaling and can be improved upon further with custom upscaling algorithms. One can create a custom node\r\nwith the required features. For my `Clouds.tscn` scene I have rendered the clouds in 3 layers with different properties and resolutions. The layers are as follows:\r\n\r\n### Horizon Clouds\r\nThese clouds are the farthest from the viewer's perspective. I've squashed the rectangle so the clouds appear compressed, faking a horizon effect. The size of the rectangle is `558 × 200`, which means:\r\n\r\n`558 × 200 = 111,600 pixels/fragments`\r\n\r\nWith the optimized shader logic averaging **~15 texture samples per fragment** (across FBM + animated noise layers), the total computation becomes:\r\n\r\n`111,600 × 15 = 1,674,000 texture samples`\r\n\r\nThe settings I've used for colours, and other exposed parameters of the shader are given below. These settings completely depend on the artistic intent of the scene, so feel free to play around!\r\n\r\n\u003Cimg src=\"/images/blog/pcg_clouds_2d/bottom_clouds_params.png\" class=\"w-full rounded-lg border border-border\" />\r\n\r\n### Middle Clouds\r\n\r\nThese clouds act as a middle-ground and move a bit faster than the background clouds to fake parallax. For this a rectangle of size `563 x 308` has been used. The clouds are slightly compressed.\r\n\r\n`563 x 308 = 173,404 pixels/fragments`\r\n\r\n`173,404 x 15 = 2,601,060 texture samples`\r\n\r\nThe shader parameters are given below:\r\n\r\n\u003Cimg src=\"/images/blog/pcg_clouds_2d/mid_clouds_params.png\" class=\"w-full rounded-lg border border-border\" />\r\n\r\n### Foreground Clouds\r\n\r\nThese clouds make up the main focal point of the scene, these clouds will have the most details visible. The are rendered across a rectangle of size `574 x 500`\r\n\r\n`574 x 500 = 287,000 pixels/fragments`\r\n\r\n`287,000 x 15 = 4,305,000 texture samples`\r\n\r\nThese clouds have a lot of detail and move the fastest across the screen, giving a fake sense of depth.  The shader parameters are given below:\r\n\r\n\u003Cimg src=\"/images/blog/pcg_clouds_2d/top_clouds_params.png\" class=\"w-full rounded-lg border border-border\" />\r\n\r\nCalculating the total samples based on the above calculations, we get:\r\n\r\n`4,305,000 + 2,601,060 + 1,674,000 = 8,580,060 samples`\r\n\r\nThis is dramatically lower than rendering at full `2160 × 6000` resolution, which would require **194,400,000 samples**. The loss in quality is also barely visible! Ingenious right? A simple trick can save\r\nyour butt at times of crunch.\r\n\r\n## Closing Section\r\n\r\nThe shader is not about realism, it's about **control**, **performance**, and **readability**.\r\n\r\nBy relying on texture-packed noise and pushing work into the vertex stage, the shader stays lightweight while remanining visually flexible.\r\n\r\nThis approach works especially well for 2D skies, parallax backgrounds, and UI-driven scenes where full volumetric clouds would be\r\nun-necessary overhead.\r\n\r\n---\r\n\r\n*Have questions about this approach? Find me on Twitter or shoot me an email :)*","src/content/blog/shaders-procedural-clouds-01.mdx","966c2f30401e54e7","shaders-procedural-clouds-01.mdx"]