[["Map",1,2,9,10,45,46],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.4","content-config-digest","fe8569cb3d167d37","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://varungupta.dev\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,31,32],"pcg-notes-01-hierarchical-tiles",{"id":11,"data":13,"body":27,"filePath":28,"digest":29,"legacyId":30,"deferredRender":25},{"title":14,"description":15,"publishedAt":16,"category":17,"tags":18,"series":23,"seriesOrder":24,"featured":25,"draft":26},"PCG Notes #1: Hierarchical Tile-Based World Generation","A deep dive into building scalable procedural terrain systems using hierarchical chunking for unlimited world size.",["Date","2025-12-01T00:00:00.000Z"],"procedural-generation",[19,20,21,22],"tutorial","unreal-engine","c++","terrain","pcg-notes",1,true,false,"Most procedural terrain systems make a critical mistake: they try to generate everything at once.\r\n\r\nThe result is predictable—either your worlds are small enough to fit in memory, or they're large but repetitive. Neither is good enough for modern open-world games.\r\n\r\nIn this article, I'll walk through a hierarchical tile-based approach that solves both problems. It's the system I developed for my recent project, and it handles worlds of essentially unlimited size while maintaining visual variety at every scale.\r\n\r\n## The Problem with Flat Tile Systems\r\n\r\nTraditional tile-based generation works something like this:\r\n\r\n```cpp\r\nfor (int x = 0; x \u003C worldSize; x++) {\r\n    for (int y = 0; y \u003C worldSize; y++) {\r\n        GenerateTile(x, y);\r\n    }\r\n}\r\n```\r\n\r\nSimple, right? But there are two fundamental issues:\r\n\r\n1. **Memory scales quadratically** — A 1000×1000 world is 1M tiles. A 10,000×10,000 world is 100M tiles.\r\n2. **No macro-scale structure** — Each tile is generated independently, so you can't have continent-scale features without expensive global passes.\r\n\r\n## Enter Hierarchical Chunking\r\n\r\nThe solution is to think in **levels of detail**, just like we do with mesh LODs. Instead of one flat grid, we use nested grids at different scales:\r\n\r\n- **Level 0**: 16×16 meter tiles (detail)\r\n- **Level 1**: 256×256 meter chunks (local features)\r\n- **Level 2**: 4096×4096 meter regions (biomes)\r\n- **Level 3**: 65536×65536 meter continents (macro structure)\r\n\r\nEach level generates based on the level above it, creating coherent large-scale structure while only loading detail where needed.\r\n\r\n## Implementation Overview\r\n\r\nHere's the core architecture:\r\n\r\n```cpp\r\nclass FHierarchicalTerrain {\r\npublic:\r\n    void GenerateRegion(FIntVector2 RegionCoord, int32 LOD);\r\n    \r\nprivate:\r\n    // Each level has its own noise configuration\r\n    TArray\u003CFNoiseConfig> LevelConfigs;\r\n    \r\n    // Cached chunks at each level\r\n    TMap\u003CFIntVector2, FTerrainChunk> ChunkCache[MAX_LEVELS];\r\n};\r\n```\r\n\r\nThe key insight is that higher levels act as **constraints** on lower levels. A Level 2 biome doesn't just influence the noise parameters—it defines the rules for what Level 1 and Level 0 can generate.\r\n\r\n## Results and Performance\r\n\r\nWith this system, I achieved:\r\n- **Unlimited world size** (tested up to 100km × 100km)\r\n- **Constant memory usage** (~200MB regardless of world size)\r\n- **Sub-frame generation** for new chunks (less than 2ms per chunk)\r\n\r\n## Next Steps\r\n\r\nIn the next PCG Notes, I'll cover the specific noise functions and how to blend between biomes without visible seams.\r\n\r\nThe key takeaway: think hierarchically. Large-scale structure should inform small-scale detail, not the other way around.\r\n\r\n---\r\n\r\n*Have questions about this approach? Find me on Twitter or drop a comment below.*","src/content/blog/pcg-notes-01-hierarchical-tiles.mdx","e38c0bb7c807f7c6","pcg-notes-01-hierarchical-tiles.mdx","shader-library-production",{"id":31,"data":33,"body":41,"filePath":42,"digest":43,"legacyId":44,"deferredRender":25},{"title":34,"description":35,"publishedAt":36,"category":37,"tags":38,"featured":26,"draft":26},"Building a Shader Library for Production","How I organize, version, and deploy shaders at scale—lessons learned from shipping tools to 500+ artists.",["Date","2025-11-18T00:00:00.000Z"],"shaders",[37,39,40],"pipelines","best-practices","When you're writing shaders for yourself, organization is optional. When you're writing shaders for a team of 50 artists, it's everything.\r\n\r\nOver the past two years, I've built and maintained shader libraries used across multiple projects. Here's what I've learned about keeping things manageable at scale.\r\n\r\n## The Folder Structure That Actually Works\r\n\r\nAfter trying various approaches, I settled on this structure:\r\n\r\n```\r\n/Shaders\r\n├── /Core\r\n│   ├── Common.hlsl          # Shared utilities\r\n│   ├── Lighting.hlsl        # Lighting functions\r\n│   └── Math.hlsl            # Math helpers\r\n├── /Materials\r\n│   ├── /Surface\r\n│   │   ├── Standard.shader\r\n│   │   ├── Foliage.shader\r\n│   │   └── Skin.shader\r\n│   └── /Effects\r\n│       ├── Dissolve.shader\r\n│       └── Hologram.shader\r\n├── /Functions\r\n│   ├── Noise.hlsl\r\n│   ├── SDF.hlsl\r\n│   └── Triplanar.hlsl\r\n└── /Debug\r\n    ├── Wireframe.shader\r\n    └── Normals.shader\r\n```\r\n\r\nThe key principles:\r\n- **Core** never depends on anything else\r\n- **Functions** can depend on Core\r\n- **Materials** can depend on both\r\n- **Debug** is separate and can depend on anything\r\n\r\n## Versioning Shaders\r\n\r\nEvery shader include has a version comment at the top:\r\n\r\n```hlsl\r\n// Noise.hlsl v2.3.1\r\n// Breaking changes in v2.0: Renamed SimplexNoise to Simplex3D\r\n```\r\n\r\nWhen I make breaking changes, I keep the old version available with a deprecation warning for one release cycle.\r\n\r\n## The Include Guard Pattern\r\n\r\nNothing breaks a shader library faster than circular includes. I use this pattern everywhere:\r\n\r\n```hlsl\r\n#ifndef NOISE_INCLUDED\r\n#define NOISE_INCLUDED\r\n\r\n// ... shader code ...\r\n\r\n#endif // NOISE_INCLUDED\r\n```\r\n\r\nSimple, but I've seen production codebases without it. Don't be that codebase.\r\n\r\n## Documentation That Artists Actually Read\r\n\r\nArtists don't read documentation. They read:\r\n- 30-second video demos\r\n- Inline parameter tooltips\r\n- Example files they can copy\r\n\r\nSo that's what I provide. Every shader ships with a demo scene and annotated examples.\r\n\r\n## Takeaways\r\n\r\n1. Structure your library by dependency, not by feature\r\n2. Version everything, even internal includes\r\n3. Use include guards religiously\r\n4. Documentation is video-first, text-second\r\n\r\nThe goal is always the same: artists shouldn't think about the shader library. They should think about their art.","src/content/blog/shader-library-production.mdx","83fe0c7ae3b8e3b9","shader-library-production.mdx","projects",["Map",47,48,68,69,88,89],"ai-landscape-generator",{"id":47,"data":49,"body":64,"filePath":65,"digest":66,"legacyId":67,"deferredRender":25},{"title":50,"description":51,"publishedAt":52,"status":53,"category":54,"technologies":55,"featured":25,"links":61,"cover":63},"AI Procedural Landscape Generator","Thesis project combining Stable Diffusion with a custom C++ OpenGL renderer for AI-guided terrain generation.",["Date","2024-08-20T00:00:00.000Z"],"completed","research",[56,57,58,59,60],"C++","OpenGL","Python","Stable Diffusion","CUDA",{"github":62},"https://github.com/VKG5/ai-landscape-generator","/images/projects/ai-landscape-cover.jpg","## Overview\r\n\r\nMy thesis project exploring the intersection of AI image generation and real-time procedural graphics. The system uses Stable Diffusion to generate landscape concepts, then reconstructs them as 3D terrain in a custom OpenGL renderer.\r\n\r\n## Research Question\r\n\r\nCan we use 2D AI-generated images as meaningful input for 3D procedural generation systems?\r\n\r\nThe answer, it turns out, is \"yes, with significant preprocessing.\"\r\n\r\n## System Architecture\r\n\r\n```\r\n┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\r\n│ Stable Diffusion│────▶│ Feature Extract │────▶│ Terrain Engine  │\r\n│ (2D Generation) │     │ (Python/OpenCV) │     │ (C++/OpenGL)    │\r\n└─────────────────┘     └─────────────────┘     └─────────────────┘\r\n```\r\n\r\n### Stage 1: AI Generation\r\nUsing Stable Diffusion with custom landscape-focused fine-tuning to generate concept images. The key was training on images with clear horizon lines and consistent lighting.\r\n\r\n### Stage 2: Feature Extraction\r\nA Python pipeline extracts:\r\n- Heightmap approximation (from luminance gradients)\r\n- Biome classification (from color clusters)\r\n- Vegetation density (from texture analysis)\r\n\r\n### Stage 3: Terrain Reconstruction\r\nThe C++ engine reconstructs 3D terrain using:\r\n- GPU-accelerated heightmap generation\r\n- Procedural detail overlay (rocks, grass, trees)\r\n- Atmospheric scattering for consistent lighting\r\n\r\n## Results\r\n\r\nThe system successfully generates coherent 3D landscapes from AI concepts, with some caveats:\r\n- Works best with landscapes that have clear depth cues\r\n- Struggles with highly stylized or abstract inputs\r\n- Generation quality depends heavily on the source image\r\n\r\n## Technical Challenges\r\n\r\n1. **Depth estimation** — 2D to 3D reconstruction is inherently ambiguous\r\n2. **Style consistency** — Matching procedural detail to AI aesthetics\r\n3. **Performance** — Real-time rendering while processing AI features\r\n\r\n## Outcome\r\n\r\n- Successfully defended thesis\r\n- Paper submitted to SIGGRAPH Asia (under review)\r\n- Open-sourced the renderer and feature extraction pipeline\r\n\r\n## Future Work\r\n\r\n- Real-time Stable Diffusion integration (currently batch-processed)\r\n- Multi-view synthesis for more accurate depth\r\n- Vegetation and structure placement using AI suggestions","src/content/projects/ai-landscape-generator.mdx","03243743588eaa65","ai-landscape-generator.mdx","virtual-puppeteering",{"id":68,"data":70,"body":84,"filePath":85,"digest":86,"legacyId":87,"deferredRender":25},{"title":71,"description":72,"publishedAt":73,"status":53,"category":74,"technologies":75,"featured":25,"links":80,"cover":83},"Virtual Puppeteering System","Real-time character animation system using OSC input devices and Unreal Engine's Control Rig for live puppeteering.",["Date","2024-03-10T00:00:00.000Z"],"tool",[76,56,77,78,79],"Unreal Engine","Control Rig","OSC","Blueprint",{"github":81,"demo":82},"https://github.com/VKG5/virtual-puppeteering","https://youtu.be/demo-link","/images/projects/virtual-puppeteering-cover.jpg","## Overview\r\n\r\nA real-time character animation system that allows performers to control digital characters using physical input devices. Built for live streaming and virtual production workflows.\r\n\r\n## The Problem\r\n\r\nTraditional animation workflows are slow. Even simple character movements require:\r\n- Keyframing every pose\r\n- Tweaking timing curves\r\n- Multiple iteration passes\r\n\r\nFor live content, this doesn't work. We needed animation that happens in real-time.\r\n\r\n## Solution\r\n\r\nThe Virtual Puppeteering System connects physical controllers (MIDI devices, custom hardware, mobile apps) to Unreal Engine characters via OSC (Open Sound Control).\r\n\r\n```\r\n┌─────────────┐    OSC    ┌─────────────┐    Control Rig    ┌─────────────┐\r\n│ Controller  │──────────▶│ Input Layer │─────────────────▶│ Character   │\r\n│ (Physical)  │           │ (Blueprint) │                   │ (Skeletal)  │\r\n└─────────────┘           └─────────────┘                   └─────────────┘\r\n```\r\n\r\n## Key Features\r\n\r\n### Modular Input Mapping\r\nAny OSC-capable device can be mapped to any control:\r\n- Sliders → Facial expressions\r\n- Buttons → Pose triggers\r\n- Accelerometers → Head tracking\r\n- Custom hardware → Custom controls\r\n\r\n### Control Rig Integration\r\nBuilt on UE5's Control Rig system for:\r\n- Physically-based secondary motion\r\n- Automatic IK solving\r\n- Blend between poses\r\n\r\n### Live Preview\r\nReal-time viewport preview with:\r\n- Sub-frame latency (~16ms)\r\n- Visual feedback for input values\r\n- Recording to sequencer\r\n\r\n## Technical Implementation\r\n\r\nThe core of the system is a custom OSC receiver in C++:\r\n\r\n```cpp\r\nvoid UOSCReceiver::OnMessageReceived(const FOSCMessage& Message)\r\n{\r\n    FString Address = Message.GetAddress();\r\n    float Value = Message.GetFloat(0);\r\n    \r\n    // Dispatch to registered handlers\r\n    if (InputHandlers.Contains(Address))\r\n    {\r\n        InputHandlers[Address].Execute(Value);\r\n    }\r\n}\r\n```\r\n\r\nThis feeds into Blueprint-exposed functions that animators can wire to any control.\r\n\r\n## Results\r\n\r\n- Used in 3 live-streamed productions\r\n- Average latency: 18ms (acceptable for live performance)\r\n- Shipped as open-source tool\r\n\r\n## Lessons Learned\r\n\r\n1. **Latency is everything** — Even 50ms feels laggy for live performance\r\n2. **Animators aren't programmers** — The Blueprint interface was essential for adoption\r\n3. **Physical controls matter** — Touch screens lack the tactile feedback performers need","src/content/projects/virtual-puppeteering.mdx","47a5c423644dd21d","virtual-puppeteering.mdx","lumi-tools",{"id":88,"data":90,"body":101,"filePath":102,"digest":103,"legacyId":104,"deferredRender":25},{"title":91,"description":92,"publishedAt":93,"status":53,"category":74,"technologies":94,"featured":25,"links":97,"cover":100},"Lumi Tools","A collection of Blender add-ons for workflow optimization—batch processing, quick scene setup, and automated exports. Used by 500+ artists.",["Date","2024-06-15T00:00:00.000Z"],[58,95,96],"Blender API","JSON",{"github":98,"download":99},"https://github.com/VKG5/lumi-tools","https://blendermarket.com/products/lumi-tools","/images/projects/lumi-tools-cover.jpg","## Overview\r\n\r\nLumi Tools is a collection of Blender add-ons I built to solve recurring pain points in my illustration and 3D workflow. What started as personal scripts grew into a toolkit now used by 500+ artists.\r\n\r\n## The Problem\r\n\r\nWorking on large illustration projects in Blender exposed several friction points:\r\n\r\n- Repetitive setup tasks for each new file (lighting rigs, render settings, collection structures)\r\n- No efficient way to batch-process reference images\r\n- Manual camera setup for turntables and beauty shots\r\n- Inconsistent export settings leading to hours of re-renders\r\n\r\nI was spending 30% of my time on tasks that added zero creative value.\r\n\r\n## The Approach\r\n\r\nRather than building one monolithic tool, I architected Lumi Tools as a modular system:\r\n\r\n1. **Core Framework** — Shared utilities for UI, settings persistence, and undo management\r\n2. **Independent Modules** — Each tool is self-contained and can be enabled/disabled\r\n3. **Preset System** — User-configurable presets that sync across projects\r\n\r\nThis approach meant I could ship useful tools quickly while building toward a cohesive system.\r\n\r\n## Tools Built\r\n\r\n| Tool | Function |\r\n|------|----------|\r\n| Quick Setup | One-click scene initialization with customizable templates |\r\n| Batch Reference | Import, organize, and process reference images |\r\n| Turntable Pro | Automated camera rigs for product shots |\r\n| Export Manager | Preset-based rendering with automatic naming and organization |\r\n| Color Palette | Extract and apply palettes from reference images |\r\n\r\n## Technical Decisions\r\n\r\n- **Python + Blender API** — Native integration, no external dependencies\r\n- **JSON Presets** — Human-readable, version-controllable settings\r\n- **Lazy Loading** — Modules load only when first accessed (improves startup time)\r\n- **Comprehensive Undo** — Every operation is fully undoable\r\n\r\n## Outcome\r\n\r\n- **~40% reduction** in setup time for new projects\r\n- **500+ active users** (based on download analytics)\r\n- **4.8/5 rating** on Blender Market\r\n\r\n## What I'd Do Differently\r\n\r\n1. **Start with user research.** I built for myself first, which worked, but talking to other artists earlier would have prioritized features better.\r\n2. **Automated testing from day one.** Manual testing doesn't scale with 20+ tools.\r\n3. **Better documentation.** Artists don't read docs, but they do watch 30-second video demos.\r\n\r\n## Key Learning\r\n\r\nGood tools are invisible—artists shouldn't think about the tool, only the work.","src/content/projects/lumi-tools.mdx","8054e63e186a6be3","lumi-tools.mdx"]