[["Map",1,2,9,10,33,34],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.4","content-config-digest","4960f797457ae201","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://varungupta.dev\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","projects",["Map",11,12],"ai-landscape-generator",{"id":11,"data":13,"body":29,"filePath":30,"digest":31,"legacyId":32,"deferredRender":25},{"title":14,"description":15,"publishedAt":16,"status":17,"category":18,"technologies":19,"featured":25,"links":26,"cover":28},"AI Procedural Landscape Generator","Thesis project combining Stable Diffusion with a custom C++ OpenGL renderer for AI-guided terrain generation.",["Date","2024-08-20T00:00:00.000Z"],"completed","research",[20,21,22,23,24],"C++","OpenGL","Python","Stable Diffusion","CUDA",true,{"github":27},"https://github.com/VKG5/ai-landscape-generator","/images/projects/ai-landscape-cover.jpg","## Overview\r\n\r\nMy thesis project exploring the intersection of AI image generation and real-time procedural graphics. The system uses Stable Diffusion to generate landscape concepts, then reconstructs them as 3D terrain in a custom OpenGL renderer.\r\n\r\n## Research Question\r\n\r\nCan we use 2D AI-generated images as meaningful input for 3D procedural generation systems?\r\n\r\nThe answer, it turns out, is \"yes, with significant preprocessing.\"\r\n\r\n## System Architecture\r\n\r\n```\r\n┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\r\n│ Stable Diffusion│────▶│ Feature Extract │────▶│ Terrain Engine  │\r\n│ (2D Generation) │     │ (Python/OpenCV) │     │ (C++/OpenGL)    │\r\n└─────────────────┘     └─────────────────┘     └─────────────────┘\r\n```\r\n\r\n### Stage 1: AI Generation\r\nUsing Stable Diffusion with custom landscape-focused fine-tuning to generate concept images. The key was training on images with clear horizon lines and consistent lighting.\r\n\r\n### Stage 2: Feature Extraction\r\nA Python pipeline extracts:\r\n- Heightmap approximation (from luminance gradients)\r\n- Biome classification (from color clusters)\r\n- Vegetation density (from texture analysis)\r\n\r\n### Stage 3: Terrain Reconstruction\r\nThe C++ engine reconstructs 3D terrain using:\r\n- GPU-accelerated heightmap generation\r\n- Procedural detail overlay (rocks, grass, trees)\r\n- Atmospheric scattering for consistent lighting\r\n\r\n## Results\r\n\r\nThe system successfully generates coherent 3D landscapes from AI concepts, with some caveats:\r\n- Works best with landscapes that have clear depth cues\r\n- Struggles with highly stylized or abstract inputs\r\n- Generation quality depends heavily on the source image\r\n\r\n## Technical Challenges\r\n\r\n1. **Depth estimation** — 2D to 3D reconstruction is inherently ambiguous\r\n2. **Style consistency** — Matching procedural detail to AI aesthetics\r\n3. **Performance** — Real-time rendering while processing AI features\r\n\r\n## Outcome\r\n\r\n- Successfully defended thesis\r\n- Paper submitted to SIGGRAPH Asia (under review)\r\n- Open-sourced the renderer and feature extraction pipeline\r\n\r\n## Future Work\r\n\r\n- Real-time Stable Diffusion integration (currently batch-processed)\r\n- Multi-view synthesis for more accurate depth\r\n- Vegetation and structure placement using AI suggestions","src/content/projects/ai-landscape-generator.mdx","03243743588eaa65","ai-landscape-generator.mdx","blog",["Map",35,36],"shaders-procedural-clouds-01",{"id":35,"data":37,"body":50,"filePath":51,"digest":52,"legacyId":53,"deferredRender":25},{"title":38,"description":39,"publishedAt":40,"category":41,"tags":42,"series":41,"seriesOrder":48,"featured":25,"draft":49},"Procedural 2D Clouds : A mathematical approach to nature","A deep dive into generating procedural clouds using maths, followed by some tips to make them appear volumetric.",["Date","2025-12-17T00:00:00.000Z"],"shaders",[43,41,44,45,46,47],"tutorial","glsl","clouds","pcg","noise",1,false,"One of the biggest mistakes I made while developing this shader was not considering the computational cost. This cost me 2 days in live-production and a huge headache, but everything worked out\r\nin the end!\r\n\r\nClouds are something that have fascinated me, you, and possibly everyone at one point in their life. Even though they appear 2D in the sky, they actually have a lot of volume and mass to them.\r\nThis also means they are computationally heavy to render/generate. My target was to use something that was NOT a volume, and what else would work if not noise! I have a separate blog for different\r\nnoises lined up, so keep out an eye ;)\r\n\r\nIn this breakdown, I'll walk through a texture-based, optimized 2D cloud shader I wrote in Godot. It's the system I developed for a recent project at Zitro, and it handles a huge amount of clouds\r\nwhile maintaining performance. The is not a \"cool clouds\" shader, rather performance-centric.\r\n\r\n\r\n## Non-Procedural Noise\r\n\r\nThe shader is built around a single-texture with **multiple textures packed** into **different channels**. If you are new to image processing, a typical image is usually comprised of `RGB channels`, usually \r\nrepresented by `JPEG`. Often times there is a fourth one as well called `Alpha`, making the image `RGBA`, with the following extensions `PNG, EXR, TARGA, TIFF`. For this shader, the channels are as\r\nfollows:\r\n\r\n- **Red Channel (R)**: Static shape mask\r\n- **Green Channel (G)**: Tiled, primary scrollable noise\r\n- **Blue Channel (B)**: Tiled, secondary scrollable noise\r\n\r\n### Why?\r\n\r\nWell, this way we avoid any expensive procedural noise functions and instead rely on **texture lookups**. These are almost always quicker, and since we have all the data in one image (AKA Texture), we only\r\nneed one lookup and one load cycle. We can simply access the various channels/sub-textures using **swizzling** in **GLSL (OpenGL Shading Language)**. This is a shorthand notation of extracting data from a container data type such as vec3.\r\n\r\n```glsl\r\nvoid fragment() {\r\n    vec4 tex = texture(TEXTURE, UV);\r\n\r\n    // Getting red, green, blue channels\r\n    vec3 tex_rgb = tex.rgb;\r\n\r\n    // Getting individual channels\r\n    float red = tex.r;\r\n    float green = tex.g;\r\n    float blue = tex.b;\r\n}\r\n```\r\n\r\nYou can grab the image below. The funky colours are due to different data present in each channel.\r\n\r\n\u003Cimg src=\"/images/blog/pcg_clouds_2d/clouds_texture.png\" class=\"w-full rounded-lg border border-border\" />\r\n\r\n\u003Cdiv class=\"grid grid-cols-3 gap-4 my-8\">\r\n    \u003Cdiv>\r\n        \u003Cimg src=\"/images/blog/pcg_clouds_2d/clouds_texture_r.png\" alt=\"Red channel - Static shape mask\" class=\"w-full rounded-lg border border-border\" />\r\n        \u003Cp class=\"text-sm text-center mt-2\" style=\"color: var(--color-text-tertiary)\">Red: Shape Mask\u003C/p>\r\n    \u003C/div>\r\n    \u003Cdiv>\r\n        \u003Cimg src=\"/images/blog/pcg_clouds_2d/clouds_texture_g.png\" alt=\"Green channel - Primary scrollable noise\" class=\"w-full rounded-lg border border-border\" />\r\n        \u003Cp class=\"text-sm text-center mt-2\" style=\"color: var(--color-text-tertiary)\">Green: Primary Noise\u003C/p>\r\n    \u003C/div>\r\n    \u003Cdiv>\r\n        \u003Cimg src=\"/images/blog/pcg_clouds_2d/clouds_texture_b.png\" alt=\"Blue channel - Secondary scrollable noise\" class=\"w-full rounded-lg border border-border\" />\r\n        \u003Cp class=\"text-sm text-center mt-2\" style=\"color: var(--color-text-tertiary)\">Blue: Secondary Noise\u003C/p>\r\n    \u003C/div>\r\n\u003C/div>\r\n\r\nIf you use [Unreal Engine](https://www.unrealengine.com/en-US)/[Quixel](https://quixel.com/en-US), or any major asset shop, this is a very common technique to save texture lookups and memory. \r\nI got introduced to the concept a few years ago, and I haven't been able to go back, it is truly the simplest optimization for any game.\r\n\r\n\r\n## Uniforms - Control Without Complexity\r\n\r\nAlright, now we know how to sample a texture. But how do we pass it on to the GPU? \r\n\r\n**Uniforms!**\r\n\r\nUniforms are special variables that act as a bridge between the CPU (Game Engine) and GPU (Shader). They are read-only parameters that remain constant across all vertices or fragments\r\nin a single draw call. Shaders run in parallel on thousands of pixels/vertices simulataneously. They cannot access you script variables directly as they are running on an entirely different\r\nprocessing environment, the GPU. Uniforms solve this by:\r\n\r\n- **Passing data once**: You set a uniform value on the CPU, and every shader invocation sees the same value\r\n- **Staying constant**: Unlike varying variables (which interpolate between vertices), uniforms don't change during a single render pass\r\n- **Being efficient**: One upload, many reads across all parallel threads\r\n\r\n### Common Use Cases\r\n\r\n- **Textures**: Passing texture samplers (like our shape mask and noise textures)\r\n- **Colors**: Theme colors, fog colors, etc.\r\n- **User settings & custom parameters**: Slider values, toggle states\r\n\r\nIn our case, we'll use a uniform to pass our noise texture and shape mask to the fragment shader, making it accessible for cloud generation. Another great thing about uniforms is that you\r\ncan make changes after compilation. A simple comaprison for uniforms can be made with **Material Instance** in Unreal Engine. The shader is **compiled once** and you can have **multiple variations** \r\nfor the same using uniforms. Analogous to **polymorphism** in **Object Oriented Programming (OOPs)**.\r\n\r\n\r\n## UV & Motion Controls\r\n\r\n```glsl\r\n// Uniforms\r\nuniform vec2 uv_scale = vec2(100, 75);\r\n\r\n// Wind control\r\nuniform vec2 direction = vec2(1.0, 0.0);\r\nuniform float cloud_scale = 1.5;\r\nuniform float speed : hint_range(-1.0, 1.0) = 0.03;\r\n```\r\n\r\nThese uniforms define the spatial scale and motion of the clouds.\r\n\r\n`uv_scale` and `cloud_scale` is used to scale the UVs. This can cause stretching/squashing along with scaling, which is desired for clouds.\r\n\r\n`direction` and `speed` together define wind movement. They have been separated for granular control over both aspect.\r\n\r\n\r\n## Lighting & Composition Controls\r\n\r\n```glsl\r\n// Artistic Controls\r\nuniform float cloud_dark = 0.5;\r\nuniform float cloud_light = 0.3;\r\nuniform float cloud_cover : hint_range(-10.0, 1.0) = 0.2;\r\nuniform float cloud_alpha : hint_range(0.0, 3.0) = 0.25;\r\nuniform float sky_tint : hint_range(0.0, 1.0) = 0.6;\r\n\r\n// Sky Hue\r\nuniform vec4 sky_colour_01 : hint_color = vec4(0.2, 0.4, 0.6, 1.0);\r\nuniform vec4 sky_colour_02 : hint_color = vec4(1.0, 0.647, 0.0, 1.0);\r\n```\r\n\r\nThese parameters are **artist-friendly**. As a technical artist and graphics engineer, it is my job to make these shaders/tools **accessible** to the artists.\r\nParameters are a great way of doing so. I sat down with an artist and managed to get some incredible results based on their tweaking! The eye and experience\r\nof an artist should be respected more than it is nowadays.\r\n\r\nInstead of **physically-based lighting**, I use simple scalar parameters to control various aspects of the cloud. This ensures smooth performance in real-time\r\nand avoids reworking noise functions or the shader.\r\n\r\n\r\n## Vertex Shader - Why It Exists Here?\r\n\r\nThe vertex shader runs only **four-times for a quad**, once per vertex. I am using a simple plane to render out the clouds for our project. This is helpful when\r\nyou look at the holistic picture. Our target resolution is native `2160 x 6000`, sometimes even more. Considering I sample the noise texture multiple times, \r\nan average of 15 per fragment, the total number of times I need to sample the texture would be:\r\n\r\n```\r\n2160 x 6000 = 12,960,000 (Total fragments)\r\n12,960,000 x 15 = 194,400,000 (Texture Samples)\r\n```\r\n\r\nPhew! That is a VERY large number. Thanks to modern GPUs, these computations do not mean a lot, but if your target is mobile devices, or weaker hardware, this becomes\r\na problem. *Like it did for me, while usinng procedural noise (Oh yes, the count was almost 30x than this).*\r\n\r\nThe vertex shader helps reduce some calculations that will remain constant throughout the lifetime of the shader.\r\n\r\n```glsl\r\n// flat varying vec2 cloud_uv : No interpolation, use vertex A's value\r\nvarying vec2 cloud_uv;\r\nvarying vec2 time_vec;\r\nvarying float aspect_ratio;\r\n\r\nvoid vertex() {\r\n\taspect_ratio = (uv_scale.y != 0.0) ? (uv_scale.x / uv_scale.y) : 1.0;\r\n\r\n\t// Fix aspect ratio so clouds look consistent on different screen sizes\r\n\tcloud_uv = UV * vec2(aspect_ratio, 1.0) * cloud_scale;\r\n\r\n\t// TIME for animating the clouds\r\n\ttime_vec = TIME * speed * direction;\r\n}\r\n```\r\n\r\nAnything that doesn't need per-pixel precision, such as *aspect ratio* or time-based offsets, is calculated here and passed down as varyings.\r\n\r\nWhile this may look like an insignificant optimization, but in shaders that are rendered across large screens, these choices add up.\r\n\r\n### Understanding the `varying` keyword\r\n\r\nThe `varying` keyword is a bridge between vertex shader and fragment shader. It gets calculated once per vertex and then gets *interpolated* across the entire surface for every\r\nfragment (pixel) that gets rendered. Consider the following code snipped:\r\n\r\n```glsl\r\n// VERTEX SHADER (runs 4 times for a quad)\r\n// Computes and send interpolated value to fragment shader\r\nvarying vec2 cloud_uv;\r\n\r\nvoid vertex() {\r\n    // This runs ONLY 4 times (once per corner of the quad)\r\n    cloud_uv = UV * vec2(aspect_ratio, 1.0) * cloud_scale;\r\n}\r\n\r\n// FRAGMENT SHADER (runs millions of times)\r\nvarying vec2 cloud_uv;\r\n\r\nvoid fragment() {\r\n    // cloud_uv is automatically interpolated between the 4 vertex values\r\n    // So every pixel gets a smoothly blended value\r\n}\r\n```\r\n\r\nWhen the GPU processed a quad (rectangle):\r\n\r\n1. **Vertex Shader** calculates `cloud_uv` at each of the 4 vertices (corners)\r\n    - Top-left corner: cloud_uv = (0.0, 1.0)\r\n    - Top-right corner: cloud_uv = (1.0, 1.0)\r\n    - Bottom-left: cloud_uv = (0.0, 0.0)\r\n    - Bottom-right: cloud_uv = (1.0, 0.0)\r\n\r\n2. The GPU automatically interpolates these values across the surface\r\n    - A pixel in the center gets cloud_uv ≈ (0.5, 0.5)\r\n    - A pixel 25% from the left gets cloud_uv ≈ (0.25, y)\r\n\r\n3. Fragment Shader receives the interpolated value for each pixel\r\n\r\n## TODO : Explain Texture Lookup and Sampling, Fractal Brownian Motion (FBM), Animated Noise, Fragment Shader, Optimization (Viewport Rendering, Upscaling)\r\n\r\n---\r\n\r\n*Have questions about this approach? Find me on Twitter or drop a comment below.*","src/content/blog/shaders-procedural-clouds-01.mdx","b479ddd6e54fdfb4","shaders-procedural-clouds-01.mdx"]